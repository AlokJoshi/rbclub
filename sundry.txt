Yes, DigitalOcean allows storing files in folders, including specific ones like an uploads folder, primarily through its S3-compatible object storage service called DigitalOcean Spaces, which supports creating directories and organizing files just like Amazon S3. You can upload files via their web control panel, API, or third-party tools, and even set up your applications (like WordPress or Django) to use Spaces for media storage, mimicking traditional local file systems. 
How it Works:
Create a Space: Think of a Space as a "bucket" for your files.
Create Folders: Within a Space, you can create folders (e.g., uploads, images) and even nested folders to organize your content.
Upload Files:
Web UI: Drag-and-drop or use the "Upload Files" button in the DigitalOcean control panel, which handles folder creation automatically.
API/CLI: Use S3-compatible tools like aws CLI, rclone, or SDKs (like boto3 for Python) to manage files and folders programmatically.
Applications: Configure your app (WordPress, Django, Node.js) to point its uploads to your DigitalOcean Space.
Access: Files are stored as objects, accessible via unique URLs, and you can set permissions to control public or private access. 
Key Benefit:
Scalability & Cost-Effectiveness: Moves storage off your main server, saving resources and scaling independently, ideal for media-heavy sites. 
So, storing files in an uploads folder on DigitalOcean is a standard and well-supported practice for managing media and user-uploaded content. 

//in utils.js file which exports checkSecret function
//which in turn is imported in server.js as follows
//const {checkSecret} = require('./utils')

require('dotenv').config();

function checkSecret(req,res,next){
  if(req.body.secret==process.env.SECRET){
    next()
  }else{
    res.sendStatus(403)
  }
}

module.exports = {
  checkSecret
}

//in upload.js

require('dotenv').config();
const { v4: uuidv4 } = require('uuid')
const mime = require('mime-types')
const sharp = require('sharp')
const { Storage } = require('@google-cloud/storage')

const deleteFromGCS = async (req,res,next) => {
  //implement the deletion from GCS

  next()
}
const downloadFromGCS = async (req, res, next)=>{
  const storage = new Storage({
    projectId: process.env.GCS_PROJECTID,
    credentials: {
      type: process.env.type,
      project_id: process.env.project_id,
      private_key_id: process.env.private_key_id,
      private_key: process.env.private_key,
      client_email: process.env.client_email,
      client_id: process.env.client_id,
      auth_uri: process.env.auth_uri,
      token_uri: process.env.type,
      auth_provider_x509_cert_url: process.env.auth_provider_x509_cert_url,
      client_x509_cert_url: process.env.client_x509_cert_url
    }
    // keyFilename: process.env.GCS_KEYFILENAME
  })
  const bucket = storage.bucket(process.env.GCS_BUCKET)

  console.log(bucket.name)
  async function listFiles() {
    // Lists files in the bucket
    const [files] = await bucket.getFiles();

    console.log('Files:');
    files.forEach(file => {
      console.log(file.name);
      console.log(file.metadata)
    });
  }
  listFiles()
  .then()
  .catch(console.error);

}
const uploadToGCS = async (req, res, next) => {

  const sharpStream = sharp(req.file.buffer)
  const promises = []

  promises.push(
    sharpStream
      .clone()
      .resize(1000)
      .toBuffer()
  )

  promises.push(
    sharpStream
      .clone()
      .resize(200)
      .toBuffer()
  )


  let buffers = [];

  Promise.all(promises)
    .then(res => {
      console.log('Sharp processing done')
      buffers = res

      //this is where we upload the data to gcs
      // console.log(`Type of buffers: ${typeof buffers}`)

      const type = mime.lookup(req.file.originalname)
      // const storage = new Storage({
      //   projectId: process.env.GCS_PROJECTID,
      //   keyFilename: process.env.GCS_KEYFILENAME
      // })

      const storage = new Storage({
        projectId: process.env.GCS_PROJECTID,
        credentials: {
          type: process.env.type,
          project_id: process.env.project_id,
          private_key_id: process.env.private_key_id,
          private_key: process.env.private_key,
          client_email: process.env.client_email,
          client_id: process.env.client_id,
          auth_uri: process.env.auth_uri,
          token_uri: process.env.type,
          auth_provider_x509_cert_url: process.env.auth_provider_x509_cert_url,
          client_x509_cert_url: process.env.client_x509_cert_url
        }
      })

      storage.getBuckets()
        .then((buckets) => console.log(`Buckets in our google cloud: ${buckets.length}`))
        .catch(err => console.log(err))

      const bucket = storage.bucket(process.env.GCS_BUCKET)

      let gcspromises = []
      req.images = new Array()
      buffers.forEach((buf, index) => {
        let blob
        let filename
        if (index == 0) {
          filename = `${req.myfolder}/${uuidv4()}.${mime.extensions[type][0]}`
        } else {
          filename = `${req.myfolder}_tn/${uuidv4()}.${mime.extensions[type][0]}`
        }
        blob = bucket.file(filename)
        console.log(`Handling the buffer ${index}`)
        let promise = new Promise((resolve, reject) => {
          let stream = blob.createWriteStream({

          })
          stream.on(`finish`, async (data) => {
            console.log(`data returned when finish takes place: ${JSON.stringify(data)}`)
            //this is to pass information to the next middleware where we can save the
            //image name to 
            req.images[index] = filename
            resolve()
          })
          stream.on('error', err => {
            console.log(`Error in uploading file: ${err}`)
            reject(err)
          })
          stream.end(buf)
        })
        gcspromises.push(promise)
      });

      Promise.all(gcspromises)
        .then(result => {
          console.log(`result returned if Promise.all succeeds: ${JSON.stringify(result)}`)
          gcspromises = []
          next()
        })
        .catch(err => {
          console.log(`Error in uploading the data: ${err}`)
          next
        })
    })
    .catch(err => console.log(`Error in Sharp processing: ${err}`))

}

let setFolderToImages = (req, res, next) => {
  req.myfolder = 'images'
  next()
}

let setFolderToProfilePics = (req, res, next) => {
  req.myfolder = 'profilepics'
  next()
}

let setFolderToFunnyImages = (req, res, next) => {
  req.myfolder = 'funnyimages'
  next()
}

let setFolderToPuzzleImages = (req, res, next) => {
  req.myfolder = 'puzzleimages'
  next()
}

module.exports = {
  deleteFromGCS,
  uploadToGCS,
  downloadFromGCS,
  setFolderToImages,
  setFolderToProfilePics,
  setFolderToFunnyImages,
  setFolderToPuzzleImages
}